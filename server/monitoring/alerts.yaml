# Prometheus Alert Rules Configuration
# SafeVideo KYC System Monitoring Alerts

groups:
  # ========================================
  # Sharegram統合アラート
  # ========================================
  - name: sharegram_integration
    interval: 30s
    rules:
      # Sharegram API高エラー率
      - alert: SharegramAPIHighErrorRate
        expr: |
          (
            sum(rate(sharegram_api_calls_total{status!="success"}[5m])) /
            sum(rate(sharegram_api_calls_total[5m]))
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          team: integration
          component: sharegram
        annotations:
          summary: "Sharegram API error rate is high"
          description: "Sharegram API error rate is {{ $value | humanizePercentage }} over the last 5 minutes"
          runbook_url: "https://wiki.safevideo.com/runbooks/sharegram-api-errors"

      # Sharegram API応答遅延
      - alert: SharegramAPIHighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(sharegram_api_duration_seconds_bucket[5m])) by (le, endpoint)
          ) > 5
        for: 10m
        labels:
          severity: warning
          team: integration
          component: sharegram
        annotations:
          summary: "Sharegram API latency is high"
          description: "95th percentile latency for {{ $labels.endpoint }} is {{ $value }}s"

      # Sharegram同期失敗
      - alert: SharegramSyncDown
        expr: sharegram_sync_status == 0
        for: 15m
        labels:
          severity: critical
          team: integration
          component: sharegram
        annotations:
          summary: "Sharegram sync is down"
          description: "Sharegram integration {{ $labels.integration_id }} has been down for 15 minutes"
          
      # Sharegram Webhook処理エラー
      - alert: SharegramWebhookFailures
        expr: |
          sum(rate(sharegram_webhooks_received_total{status="failed"}[10m])) > 5
        for: 5m
        labels:
          severity: warning
          team: integration
          component: sharegram
        annotations:
          summary: "High Sharegram webhook failure rate"
          description: "{{ $value }} webhook failures per minute"

  # ========================================
  # KYCアラート
  # ========================================
  - name: kyc_monitoring
    interval: 30s
    rules:
      # KYC検証失敗率
      - alert: KYCHighFailureRate
        expr: |
          (
            sum(rate(kyc_verifications_total{status="failed"}[15m])) /
            sum(rate(kyc_verifications_total[15m]))
          ) > 0.2
        for: 10m
        labels:
          severity: critical
          team: compliance
          component: kyc
        annotations:
          summary: "KYC verification failure rate is high"
          description: "KYC failure rate is {{ $value | humanizePercentage }} over the last 15 minutes"
          impact: "New performer registrations may be blocked"

      # KYC検証遅延
      - alert: KYCVerificationSlow
        expr: |
          histogram_quantile(0.95,
            sum(rate(kyc_verification_duration_seconds_bucket[10m])) by (le, provider)
          ) > 120
        for: 15m
        labels:
          severity: warning
          team: compliance
          component: kyc
        annotations:
          summary: "KYC verification is slow"
          description: "95th percentile verification time for {{ $labels.provider }} is {{ $value }}s"

      # KYC期限切れ警告
      - alert: KYCExpirationWarning
        expr: kyc_expiration_warnings{days_until_expiry="7"} > 50
        for: 1h
        labels:
          severity: warning
          team: compliance
          component: kyc
        annotations:
          summary: "Many KYC verifications expiring soon"
          description: "{{ $value }} performers have KYC expiring within 7 days"
          action: "Review and initiate re-verification process"

      # KYC期限切れ緊急
      - alert: KYCExpirationCritical
        expr: kyc_expiration_warnings{days_until_expiry="7"} > 100
        for: 30m
        labels:
          severity: critical
          team: compliance
          component: kyc
        annotations:
          summary: "Critical number of KYC expirations"
          description: "{{ $value }} performers have KYC expiring within 7 days"

  # ========================================
  # Performerアラート
  # ========================================
  - name: performer_monitoring
    interval: 30s
    rules:
      # 登録失敗率
      - alert: PerformerRegistrationFailures
        expr: |
          (
            sum(rate(performer_registrations_total{status="failed"}[30m])) /
            sum(rate(performer_registrations_total[30m]))
          ) > 0.1
        for: 15m
        labels:
          severity: warning
          team: platform
          component: performer
        annotations:
          summary: "High performer registration failure rate"
          description: "Registration failure rate is {{ $value | humanizePercentage }}"

      # ドキュメントアップロード失敗
      - alert: DocumentUploadFailures
        expr: |
          sum(rate(performer_document_uploads_total{status="failed"}[15m])) > 10
        for: 10m
        labels:
          severity: warning
          team: platform
          component: performer
        annotations:
          summary: "High document upload failure rate"
          description: "{{ $value }} document upload failures per minute"

  # ========================================
  # Webhookアラート
  # ========================================
  - name: webhook_monitoring
    interval: 30s
    rules:
      # Webhook送信失敗
      - alert: WebhookDeliveryFailures
        expr: |
          (
            sum(rate(webhooks_sent_total{status="failed"}[10m])) /
            sum(rate(webhooks_sent_total[10m]))
          ) > 0.05
        for: 10m
        labels:
          severity: warning
          team: platform
          component: webhook
        annotations:
          summary: "Webhook delivery failure rate is high"
          description: "Webhook failure rate is {{ $value | humanizePercentage }}"

      # Webhook配信遅延
      - alert: WebhookDeliveryDelay
        expr: |
          histogram_quantile(0.95,
            sum(rate(webhook_delivery_delay_seconds_bucket[10m])) by (le, event_type)
          ) > 30
        for: 15m
        labels:
          severity: warning
          team: platform
          component: webhook
        annotations:
          summary: "Webhook delivery is delayed"
          description: "95th percentile delay for {{ $labels.event_type }} is {{ $value }}s"

      # 過剰なWebhookリトライ
      - alert: ExcessiveWebhookRetries
        expr: |
          sum(rate(webhook_retries_total{attempt_number="3"}[1h])) > 100
        for: 30m
        labels:
          severity: warning
          team: platform
          component: webhook
        annotations:
          summary: "Excessive webhook retries detected"
          description: "{{ $value }} webhooks required maximum retries in the last hour"

  # ========================================
  # APIパフォーマンスアラート
  # ========================================
  - name: api_performance
    interval: 30s
    rules:
      # API高エラー率
      - alert: APIHighErrorRate
        expr: |
          (
            sum(rate(api_requests_total{status_code=~"5.."}[5m])) by (route) /
            sum(rate(api_requests_total[5m])) by (route)
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          team: platform
          component: api
        annotations:
          summary: "API error rate is high for {{ $labels.route }}"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # API応答遅延
      - alert: APIHighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(api_request_duration_seconds_bucket[5m])) by (le, route)
          ) > 2
        for: 10m
        labels:
          severity: warning
          team: platform
          component: api
        annotations:
          summary: "API latency is high for {{ $labels.route }}"
          description: "95th percentile latency is {{ $value }}s"

      # API高負荷
      - alert: APIHighLoad
        expr: api_concurrent_requests > 1000
        for: 5m
        labels:
          severity: warning
          team: platform
          component: api
        annotations:
          summary: "API is under high load"
          description: "{{ $value }} concurrent requests"

  # ========================================
  # システムエラーアラート
  # ========================================
  - name: system_errors
    interval: 30s
    rules:
      # 高エラー率
      - alert: HighErrorRate
        expr: |
          sum(rate(errors_total[5m])) by (category) > 10
        for: 5m
        labels:
          severity: warning
          team: platform
          component: system
        annotations:
          summary: "High error rate in {{ $labels.category }}"
          description: "{{ $value }} errors per second"

      # クリティカルエラー
      - alert: CriticalErrors
        expr: |
          sum(rate(errors_total{severity="critical"}[5m])) > 1
        for: 2m
        labels:
          severity: critical
          team: platform
          component: system
          pager: true
        annotations:
          summary: "Critical errors detected"
          description: "{{ $value }} critical errors per second in {{ $labels.category }}"

      # セキュリティエラー
      - alert: SecurityErrors
        expr: |
          sum(rate(errors_total{category="authentication"}[10m])) > 50
        for: 5m
        labels:
          severity: critical
          team: security
          component: system
        annotations:
          summary: "High authentication error rate"
          description: "{{ $value }} authentication errors per second"

  # ========================================
  # データベースアラート
  # ========================================
  - name: database_monitoring
    interval: 30s
    rules:
      # データベース接続プール枯渇
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          (
            database_connection_pool_size{state="waiting"} /
            sum(database_connection_pool_size) by (job)
          ) > 0.8
        for: 5m
        labels:
          severity: critical
          team: platform
          component: database
        annotations:
          summary: "Database connection pool is almost exhausted"
          description: "{{ $value | humanizePercentage }} of connections are waiting"

      # データベース接続エラー
      - alert: DatabaseConnectionErrors
        expr: |
          sum(rate(errors_total{category="database"}[5m])) > 5
        for: 5m
        labels:
          severity: critical
          team: platform
          component: database
        annotations:
          summary: "High database error rate"
          description: "{{ $value }} database errors per second"

  # ========================================
  # ビジネスメトリクスアラート
  # ========================================
  - name: business_metrics
    interval: 60s
    rules:
      # コンテンツ承認率低下
      - alert: LowContentApprovalRate
        expr: content_approval_rate{time_period="24h"} < 70
        for: 1h
        labels:
          severity: warning
          team: compliance
          component: business
        annotations:
          summary: "Content approval rate is low"
          description: "Approval rate is {{ $value }}% for {{ $labels.content_type }}"

      # アクティブPerformer減少
      - alert: ActivePerformersDecreasing
        expr: |
          (
            active_performers_total - 
            active_performers_total offset 24h
          ) / active_performers_total offset 24h < -0.1
        for: 2h
        labels:
          severity: warning
          team: business
          component: business
        annotations:
          summary: "Active performers decreasing"
          description: "Active performers decreased by {{ $value | humanizePercentage }} in 24h"

# ========================================
# 通知ルール設定
# ========================================
# Alertmanagerで以下の通知ルールを設定：
#
# routes:
#   - match:
#       severity: critical
#     receiver: pagerduty
#     continue: true
#   
#   - match:
#       severity: critical
#     receiver: slack-critical
#     
#   - match:
#       severity: warning
#     receiver: slack-warning
#     
#   - match:
#       team: security
#     receiver: security-team
#     
#   - match:
#       team: compliance
#     receiver: compliance-team
#
# receivers:
#   - name: pagerduty
#     pagerduty_configs:
#       - service_key: <SERVICE_KEY>
#         severity: critical
#         
#   - name: slack-critical
#     slack_configs:
#       - api_url: <WEBHOOK_URL>
#         channel: '#alerts-critical'
#         
#   - name: slack-warning
#     slack_configs:
#       - api_url: <WEBHOOK_URL>
#         channel: '#alerts-warning'